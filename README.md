# RL4LLM

## Reinforcement Algorithm
| Title | Affiliation | Code |  Method |
|-----|-----|-----|-----|
| [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/abs/1707.06347) | OpenAI 2017 | x | RLHFä¸­çš„ç»å…¸å¼ºåŒ–ç®—æ³•ã€‚ğŸ”¥é‡‡æ ·é‡è¦æ€§é‡‡æ ·ä» online RL è½¬å‘ offline RLï¼Œå¢å¼ºè®­ç»ƒæ•ˆç‡ã€‚ğŸ”¥é‡‡ç”¨ clip æœºåˆ¶ç»´æŒæ¢¯åº¦æ›´æ–°â€œæ­¥é•¿â€ï¼Œé˜²æ­¢é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚|
| [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/abs/2305.18290) | Stanford 2024 | x | ä¾æ‰˜ Bradley-Terryï¼Œéšå¼åœ°ä¸PPOå®ç°ç›¸åŒçš„ä¼˜åŒ–ç›®æ ‡ã€‚ğŸ”¥é‡‡ç”¨ç›´æ¥åå¥½å­¦ä¹ ä»£æ›¿æ˜¾ç¤ºçš„å¥–åŠ±å»ºæ¨¡ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚ |
| [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (GRPO)](https://arxiv.org/abs/2402.03300) | DeepSeek 2024 | [code](https://github.com/deepseek-ai/DeepSeek-Math) | åœ¨PPOçš„åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚ğŸ”¥èˆå¼ƒäº† Critic Modelï¼Œé’ˆå¯¹åŒä¸€ question é‡‡æ ·ä¸€ç»„ responsesï¼Œå¹¶å¯¹ responses çš„ rewards è¿›è¡Œç»„å†…å½’ä¸€åŒ–å¾—åˆ°æ¯ä¸ª response å¯¹åº”çš„ advantageã€‚ğŸ”¥åœ¨ç›®æ ‡å‡½æ•°ä¸­å¼•å…¥äº†åå‘KLæ•£åº¦ã€‚ |
| [Break the Chain: Large Language Models Can be Shortcut Reasoners](https://arxiv.org/pdf/2406.06580v1) | arXiv 2024.6.4 | x | Propose zero-shot prompting strategies to encourage the use of shortcuts. Introduce ShortcutQA, a dataset designed to evaluate reasoning through shortcuts. |
| [Token-Budget-Aware LLM Reasoning](https://arxiv.org/pdf/2412.18547) | arXiv 2025.2.17 | [code](https://github.com/GeniusHTX/TALE) | Use LLM to estimate the optimal CoT budget and then use prompt to generate shorter CoT. |
| [How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach](https://arxiv.org/pdf/2503.01141) | arXiv 2025.3.3 | [code](https://github.com/Compressed-CoT/compressed-cot) | Find the relationship between CoT length and accuracy and; there is an intrinsic shortest CoT for successfully solving each task. |
| [Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/pdf/2502.13260) | arXiv 2025.2.28 | x | Use PPL to identify critical reasoning steps. Refine demonstration examples in few-shot CoT or finetuning the model using selected examples that include only critical steps. |
