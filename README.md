# RL4LLM

## Reinforcement Algorithm
| Title | Affiliation | Code |  Method |
|-----|-----|-----|-----|
| [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/abs/1707.06347) | OpenAI 2017.7 | x | RLHF ä¸­çš„ç»å…¸å¼ºåŒ–ç®—æ³•ã€‚ğŸ”¥é‡‡æ ·é‡è¦æ€§é‡‡æ ·ä» online RL è½¬å‘ offline RLï¼Œå¢å¼ºè®­ç»ƒæ•ˆç‡ã€‚ğŸ”¥é‡‡ç”¨ clip æœºåˆ¶ç»´æŒæ¢¯åº¦æ›´æ–°â€œæ­¥é•¿â€ï¼Œé˜²æ­¢é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚|
| [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/abs/2305.18290) | Stanford 2023.5 | x | ä¾æ‰˜ Bradley-Terryï¼Œéšå¼åœ°ä¸ PPO å®ç°ç›¸åŒçš„ä¼˜åŒ–ç›®æ ‡ã€‚ğŸ”¥é‡‡ç”¨ç›´æ¥åå¥½å­¦ä¹ ä»£æ›¿æ˜¾ç¤ºçš„å¥–åŠ±å»ºæ¨¡ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚ |
| [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (GRPO)](https://arxiv.org/abs/2402.03300) | DeepSeek 2024.2 | [code](https://github.com/deepseek-ai/DeepSeek-Math) | åœ¨ PPO çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚ğŸ”¥èˆå¼ƒäº† Critic Modelï¼Œé’ˆå¯¹åŒä¸€ question é‡‡æ ·ä¸€ç»„ responsesï¼Œå¹¶å¯¹ responses çš„ rewards è¿›è¡Œç»„å†…å½’ä¸€åŒ–å¾—åˆ°æ¯ä¸ª response å¯¹åº”çš„ advantageã€‚ğŸ”¥åœ¨ç›®æ ‡å‡½æ•°ä¸­å¼•å…¥äº†åå‘KLæ•£åº¦ã€‚ |
| [Understanding R1-Zero-Like Training: A Critical Perspective (Dr.GRPO)](https://arxiv.org/abs/2503.20783) | NUS 2025.3 | [code](https://github.com/sail-sg/understand-r1-zero) | åœ¨ GRPO çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚ğŸ”¥è®¤ä¸º GRPO ä¸­å­˜åœ¨ä¸¤æ–¹é¢çš„ bias: 1)GRPO é’ˆå¯¹ responses çš„é•¿åº¦å–å¹³å‡ä¼šå¼•å…¥ length biasï¼Œå¯¼è‡´æŸå¤±å‡½æ•°é¼“åŠ±æ›´çŸ­çš„ä¼˜åŠ¿è¾“å‡ºå’Œæ›´é•¿çš„åŠ£åŠ¿è¾“å‡ºï¼›2)GRPOå¯¹rewardsè¿›è¡Œç»„å†…å½’ä¸€åŒ–æ—¶é™¤ä»¥æ ‡å‡†å·®ä¼šå¼•å…¥ Question-level difficulty biasï¼Œå³æ›´éš¾çš„é—®é¢˜å¾€å¾€ä¼´éšæ›´å¤§çš„æ–¹å·®ï¼Œä»è€Œè·å¾—æ›´å°çš„æ¢¯åº¦æ›´æ–°ã€‚ğŸ”¥æ”¹è¿›ï¼šç›´æ¥èˆå¼ƒæ‰äº†å¯¼è‡´ä¸¤ç§ bias çš„ç®—å­ã€‚ |
| [DAPO: An Open-Source LLM Reinforcement Learning System at Scale (DAPO)](https://arxiv.org/abs/2503.14476) | ByteDance Seed 2025.3 | [code](https://github.com/volcengine/verl) | åœ¨GRPOçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚ğŸ”¥Clip-Higher: é¿å… entropy collapseã€‚ğŸ”¥Dynamic Sampling: æå‡è®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ã€‚ğŸ”¥Token-Level Policy Gradient Loss: é€‚ç”¨äº long-CoT RL åœºæ™¯ã€‚ğŸ”¥Overlong Reward Shaping:å‡å°‘ reward noiseã€‚ |

## Reward Modeling
| Title | Publish/Affiliation | Code |  Method |
|-----|-----|-----|-----|
| [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168) | OpenAI 2021.11 | x | ğŸ¯ORM & Human AnnotationğŸ¯ã€‚é€šè¿‡äººç±»æ ‡æ³¨çš„åŸºäºç»“æœçš„ç›‘ç£ä¿¡å·è®­ç»ƒä¸€ä¸ª Outcome-based Verifier/Reward Model |
| [Letâ€™s Verify Step by Step](https://arxiv.org/abs/2305.20050) | OpenAI 2023.5 | x | ğŸ¯PRM & Human AnnotationğŸ¯ã€‚é€šè¿‡äººç±»æ ‡æ³¨çš„åŸºäºè¿‡ç¨‹çš„ç›‘ç£ä¿¡å·è®­ç»ƒä¸€ä¸ª Process-based Verifier/Reward Model |
| [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935) | ACL 2024 | x | ğŸ¯PRM & Model AnnotationğŸ¯ã€‚ä½¿ç”¨LLMä»æŸä¸€ step å¼€å§‹ç»­å†™ç›´è‡³æœ€ç»ˆç­”æ¡ˆï¼Œé‡å¤å¤šæ¬¡ï¼ˆè’™ç‰¹å¡æ´›æ€æƒ³ï¼‰ï¼Œå°†ç»­å†™æˆåŠŸç‡ä½œä¸ºå½“å‰ step çš„ rewardã€‚|
| [Understanding Chain-of-Thought in LLMs through Information Theory](https://arxiv.org/abs/2411.11984) | ByteDance 2024.11 | x | ğŸ¯PRM & Model AnnotationğŸ¯ã€‚å°†ä¿¡æ¯è®ºå¼•å…¥ Reward Modelingï¼Œå°†å½“å‰ step åˆ°ä¸‹ä¸€ step çš„ä¿¡æ¯å¢ç›Šä½œä¸ºå½“å‰ step çš„rewardã€‚ï¼ˆä½†æ˜¯å¯¹äºä¿¡æ¯å¢ç›Šçš„å®šä¹‰å¹¶ä¸æ˜¯å¾ˆæœ‰è¯´æœåŠ›ã€‚ã€‚ã€‚ï¼‰ |
| [Process Reinforcement through Implicit Rewards](https://arxiv.org/abs/2502.01456) | Tsinghua 2025.2 | [code](https://github.com/PRIME-RL/PRIME) | ğŸ¯PRM & Human AnnotationğŸ¯ã€‚ |
| [From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge](https://arxiv.org/abs/2411.16594) | arXiv 2024.11 | x |ğŸ¯LLM-as-JudgeğŸ¯ã€‚ï¼ˆSurveyï¼‰ |
| [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495) | DeepSeek 2025.4 | x | ğŸ¯Generative Reward ModelğŸ¯ã€‚æå‡ºäº†ä¸€ç§åä¸º Self-Principled Critique Tuning(SPCT) çš„æŠ€æœ¯æ¥é€šè¿‡ online RL çš„æ–¹å¼æ¥å¢å¼º GRMï¼Œå¾—åˆ°äº†DeepSeek-GRMã€‚ |
| [What Makes a Reward Model a Good Teacher? An Optimization Perspective](https://arxiv.org/abs/2503.15477) | Princeton 2025.3 | x | ğŸ¯Theoretical PerspectiveğŸ¯ã€‚ä»reward variance çš„è§†è§’æ¢ç©¶å¦‚ä½•ä¼˜åŒ– Reward Model çš„è®­ç»ƒã€‚ |
| [Self-Generated Critiques Boost Reward Modeling for Language Models](https://arxiv.org/abs/2411.16646) | Meta GenAI 2025.2 | x | ğŸ¯Generative Reward ModelğŸ¯ã€‚æå‡ºäº† Critic-RMï¼Œä½¿ç”¨ self-generatedï¼Œhigh-quality critiques æ¥è®­ç»ƒ Reward Modelã€‚ |
| [Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning](https://openreview.net/forum?id=A6Y7AqlzLW) | ICLR 2025 | x | ğŸ¯PRM & Automated AnnotationğŸ¯ã€‚|

## Complete training pipeline
| Title | Publish/Affiliation | Code |  Method |
|-----|-----|-----|-----|
| [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168) | OpenAI 2021.11 | x | ğŸ¯ORM & Human AnnotationğŸ¯ã€‚é€šè¿‡äººç±»æ ‡æ³¨çš„åŸºäºç»“æœçš„ç›‘ç£ä¿¡å·è®­ç»ƒä¸€ä¸ª Outcome-based Verifier/Reward Model |
| [Letâ€™s Verify Step by Step](https://arxiv.org/abs/2305.20050) | OpenAI 2023.5 | x | ğŸ¯PRM & Human AnnotationğŸ¯ã€‚é€šè¿‡äººç±»æ ‡æ³¨çš„åŸºäºè¿‡ç¨‹çš„ç›‘ç£ä¿¡å·è®­ç»ƒä¸€ä¸ª Process-based Verifier/Reward Model |

## Application of RL in LLMs
| Title | Publish/Affiliation | Code |  Method |
|-----|-----|-----|-----|
| [Training Language Models to Self-Correct via Reinforcement Learning](https://openreview.net/forum?id=CjwERcAU7w) | ICLR 2025 | x | é€šè¿‡RLæ¥æå‡LLMçš„è‡ªæˆ‘çº é”™èƒ½åŠ› |

## RL-Agent
| Title | Publish/Affiliation | Code |  Method |
|-----|-----|-----|-----|
| [Search-o1: Agentic Search-Enhanced Large Reasoning Models](https://arxiv.org/abs/2501.05366) | 2025.1 arxiv | [code](https://search-o1.github.io/) | Agentic Search |
| [Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516) | 2025.3 arxiv | [code](https://github.com/PeterGriffinJin/Search-R1) | Agentic Search |
| [R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.05592) | 2025.3 arxiv | [code](https://github.com/RUCAIBox/R1-Searcher) | Agentic Search |
